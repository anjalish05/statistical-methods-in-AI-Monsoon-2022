{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d1724f",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46240d43",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "- Write modular code with relevant docstrings and comments for you to be able to use\n",
    "functions you have implemented in future assignments.\n",
    "- All theory questions and observations must be written in a markdown cell of your jupyter notebook.You can also add necessary images in `imgs/` and then include it in markdown. Any other submission method for theoretical question won't be entertained.\n",
    "- Start the assignment early, push your code regularly and enjoy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79190744",
   "metadata": {},
   "source": [
    "<b>Name: </b> Anjali Singh <br>\n",
    "<b>Roll No.: </b>2020102004 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d6a65",
   "metadata": {},
   "source": [
    "### Question 1 Optimal DT from table\n",
    "**[20 points]**\\\n",
    "We will use the dataset below to learn a decision tree which predicts if people pass machine\n",
    "learning (Yes or No), based on their previous GPA (High, Medium, or Low) and whether or\n",
    "not they studied. \n",
    "\n",
    "| GPA | Studied | Passed |\n",
    "|:---:|:-------:|:------:|\n",
    "|  L  |    F    |    F   |\n",
    "|  L  |    T    |    T   |\n",
    "|  M  |    F    |    F   |\n",
    "|  M  |    T    |    T   |\n",
    "|  H  |    F    |    T   |\n",
    "|  H  |    T    |    T   |\n",
    "    \n",
    " For this problem, you can write your answers using $log_2$\n",
    ", but it may be helpful to note\n",
    "that $log_2 3 â‰ˆ 1.6$.\n",
    "\n",
    "---\n",
    "1. What is the entropy H(Passed)?\n",
    "2. What is the entropy H(Passed | GPA)?\n",
    "3. What is the entropy H(Passed | Studied)?\n",
    "4. Draw the full decision tree that would be learned for this dataset. You do\n",
    "not need to show any calculations.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81925d87",
   "metadata": {},
   "source": [
    "<b> 1. What is the entropy H(passed)? </b><br>\n",
    "\n",
    "$ H(passed) = -p_{true}log_2p_{true} - p_{false}log_2p_{false}  \\newline\n",
    "p_{true} = 4/6 = 2/3; \\newline\n",
    "p_{false} = 2/6 = 1/3; \\newline\n",
    "\\implies H(passed) = -((2/3)log_2(2/3) + (1/3)log_2(1/3)) \\newline\n",
    "$\n",
    "<br>\n",
    "After calculations, we get <br>\n",
    "$ H(passed) = 0.934 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f9257",
   "metadata": {},
   "source": [
    "<b> 2. What is the entropy H(passed|GPA)? </b><br>\n",
    "$$ H(X|Y) = \\sum_{x\\epsilon X}\\sum_{y\\epsilon Y} P(x, y)log_2 (P(y)/P(x, y))\n",
    "\n",
    "\\newline\n",
    " \\implies H(passed | GPA) = \\sum \\sum P(passed, GPA)log_2 P(GPA)/P(passed, GPA) \n",
    " \\newline\n",
    " \\implies P(passed, GPA) = 2/6 * 1/2 \\space for \\space GPA = L, M \\newline\n",
    " \\implies P(passed, GPA) = 2/6 * 1 \\space for \\space GPA = H \\newline\n",
    " \\implies H(passed | GPA) = ((2/6*1/2)log_2(1/6)/(1/6)) * 4 + ((2/6 * 1)log_2(2/6)/(2/6)) \\newline\n",
    " \\implies H(passed | GPA) = 0.667\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82f69b",
   "metadata": {},
   "source": [
    "<b> 3. What is entropy H(passed | Studied)? </b><br>\n",
    "$$ H(X|Y) = \\sum_{x\\epsilon X}\\sum_{y\\epsilon Y} P(x, y)log_2 (P(y)/P(x, y))\n",
    "\\newline\n",
    "\\implies H(passed | studied) = ((1/2 * 1/2)log_2(1/2)/(1/4)) * 4 + ((1/2 * 1)log_2(1/2)/(1/2)) * 2 \\newline\n",
    "\\implies H(passed | studied) = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df835126",
   "metadata": {},
   "source": [
    "<b> 4. Draw the full decision tree that would be learned for this dataset. </b><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cf408",
   "metadata": {},
   "source": [
    "### Question 2 DT loss functions\n",
    "**[10 points]**\n",
    "1. Explain Gini impurity and Entropy. \n",
    "2. What are the min and max values for both Gini impurity and Entropy\n",
    "3. Plot the Gini impurity and Entropy for $p\\in[0,1]$.\n",
    "4. Multiply Gini impurity by a factor of 2 and overlay it over entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd6c56",
   "metadata": {},
   "source": [
    "<b>1. Explain Gini impurity and Entropy. </b><br>\n",
    "<b>Entropy: </b> It is one of the ways to measure the impurity in the collection of data, i.e., non-homogeneity. It returns the information about the dataset that how impure the data is. \n",
    "Given a collection of dataset $S$, and the probability for classes is represented by $p_i$, entropy can be calculated as: - <br>\n",
    "$ Entropy = -\\sum p_ilog_2(p_i) $ \n",
    "<br><br>\n",
    "<b> Gini Impurity: </b> It is a measurement used to build decision trees to determine how the features of a dataset should split nodes to form the tree. It usually $ \\epsilon\\space [0, 0.5] $. <br>\n",
    "As taught in the class, it can also be referred to as an extended variance impurity to three classes: $ p(w_1) p(w_2) p(w_3) $ <br>\n",
    "$ i(N) = p(w_1)p(w_2) + p(w_2)p(w_3) + p(w_3)p(w_1) $ <br>\n",
    "For a dataset $D$ that contains samples from $k$ classes. The probability of samples belonging to class $i$ at a given node can be denoted as $p_i$. Then, <br>\n",
    "$ gini(D) = 1 - \\sum_{i=1}^{k}p_i^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93c876",
   "metadata": {},
   "source": [
    "<b>2. What are the min and max values for both Gini impurity and Entropy? </b><br>\n",
    "Min value of gini impurity: 0 <br>\n",
    "max value of gini impurity: 0.5 <br>\n",
    "min value of entropy: 0 <br>\n",
    "max value of entropy: 1 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340a7d7",
   "metadata": {},
   "source": [
    "<b>3. Plot the Gini impurity and Entropy for $p\\in[0,1]$ </b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366fd0a3",
   "metadata": {},
   "source": [
    "<b>4. Multiply Gini impurity by a factor of 2 and overlay it over entropy. </b><br>\n",
    "$ gini \\epsilon [0, 0.5] $ <br>\n",
    "$ entropy \\epsilon [0, 1] $ <br>\n",
    "\n",
    "$ gini*2 \\space\\epsilon\\space [0, 1] $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a406ac",
   "metadata": {},
   "source": [
    "### Question 3 Training a Decision Tree  \n",
    "**[40 points]**\n",
    "\n",
    "You can download the spam dataset from the link given below. This dataset contains feature vectors and the lables of Spam/Non-Spam mails. \n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
    "\n",
    "**NOTE: The last column in each row represents whether the mail is spam or non spam**\\\n",
    "Although not needed, incase you want to know what the individual columns in the feature vector means, you can read it in the documentation given below.\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de757a",
   "metadata": {},
   "source": [
    "**Download the data and load it from the code given below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8289a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163c0c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 58)\n",
      "[[0.000e+00 6.400e-01 6.400e-01 ... 6.100e+01 2.780e+02 1.000e+00]\n",
      " [2.100e-01 2.800e-01 5.000e-01 ... 1.010e+02 1.028e+03 1.000e+00]\n",
      " [6.000e-02 0.000e+00 7.100e-01 ... 4.850e+02 2.259e+03 1.000e+00]\n",
      " ...\n",
      " [3.000e-01 0.000e+00 3.000e-01 ... 6.000e+00 1.180e+02 0.000e+00]\n",
      " [9.600e-01 0.000e+00 0.000e+00 ... 5.000e+00 7.800e+01 0.000e+00]\n",
      " [0.000e+00 0.000e+00 6.500e-01 ... 5.000e+00 4.000e+01 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######################\n",
    "# Your code goes here #\n",
    "\n",
    "raw_data = pd.read_csv(\"spambase.data\", header=None)\n",
    "data = raw_data.values\n",
    "print(data.shape)\n",
    "print(data)\n",
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffee80e",
   "metadata": {},
   "source": [
    "You can try to normalize each column (feature) separately with either one of the following ideas. **Do not normalize labels**.\n",
    "- Shift-and-scale normalization: substract the minimum, then divide by new maximum. Now all values are between 0-1\n",
    "- Zero mean, unit variance : substract the mean, divide by the appropriate value to get variance=1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8716e1",
   "metadata": {},
   "source": [
    "Normalization formula used here: <br>\n",
    "[Shift-and-scale normalization] <br>\n",
    "$$ x_{norm} = (x-x_{min})/(x_{max}-x_{min}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add119f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML algos tend to perform better or converge faster when the different features (variables)\n",
    "# are on a smaller scale. \n",
    "\n",
    "def normalize(data):\n",
    "    # normalize a 2D matrix dataset \n",
    "    # return a normalized 2D matrix dataset\n",
    "    # YOUR CODE HERE\n",
    "    row, col = data.shape\n",
    "    for i in range(col-1):\n",
    "        data[:,i] = (data[:,i] - data[:,i].min())/(data[:,i].max() - data[:,i].min())\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0aff341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data Shape:  (4601, 58)\n",
      "[[0.00000000e+00 4.48179272e-02 1.25490196e-01 ... 6.00720865e-03\n",
      "  1.74873737e-02 1.00000000e+00]\n",
      " [4.62555066e-02 1.96078431e-02 9.80392157e-02 ... 1.00120144e-02\n",
      "  6.48358586e-02 1.00000000e+00]\n",
      " [1.32158590e-02 0.00000000e+00 1.39215686e-01 ... 4.84581498e-02\n",
      "  1.42550505e-01 1.00000000e+00]\n",
      " ...\n",
      " [6.60792952e-02 0.00000000e+00 5.88235294e-02 ... 5.00600721e-04\n",
      "  7.38636364e-03 0.00000000e+00]\n",
      " [2.11453744e-01 0.00000000e+00 0.00000000e+00 ... 4.00480577e-04\n",
      "  4.86111111e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.27450980e-01 ... 4.00480577e-04\n",
      "  2.46212121e-03 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "data_norm = normalize(data)\n",
    "print('Normalized Data Shape: ', data_norm.shape)\n",
    "print(data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef858082",
   "metadata": {},
   "source": [
    "1. Split your data into train 80% and test dataset 20% \n",
    "2. **[BONUS]** Visualize the data using PCA . You can reduce the dimension of the data if you want. Bonus marks if this increases your accuracy.\n",
    "\n",
    "*NOTE: If you are applying PCA or any other type of dimensionality reduction, do it before splitting the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817244db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "# Your code goes here #\n",
    "\n",
    "def pca(data, num_components):\n",
    "    # finding eigen values and eigen vectors using pca method for the dataset\n",
    "    # returning the first num_components of eigen vectors\n",
    "\n",
    "    row, col = data.shape\n",
    "    mean = np.mean(data, axis=0)\n",
    "    data = data - mean\n",
    "    cov = np.cov(data, rowvar=False)\n",
    "\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(cov)\n",
    "    eigen_vectors = eigen_vectors[:, np.argsort(eigen_values)[::-1]]\n",
    "    eigen_vectors = eigen_vectors[:, :num_components]\n",
    "    \n",
    "    data_reduced = np.dot(data, eigen_vectors)\n",
    "\n",
    "    return data_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "469325b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.60371339  0.00161239]\n",
      " [ 0.62378346 -0.01153851]\n",
      " [ 0.63452676  0.01365266]\n",
      " ...\n",
      " [-0.39005723 -0.07295247]\n",
      " [-0.39012632 -0.07298202]\n",
      " [-0.37882707 -0.08828848]]\n"
     ]
    }
   ],
   "source": [
    "# implementing the above pca function to reduce the dimension of the given dataset\n",
    "\n",
    "data_pca = pca(data_norm, 2)\n",
    "\n",
    "print(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc925371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " [[ 0.61878897  0.00203542]\n",
      " [-0.39833867 -0.05839492]\n",
      " [-0.37017848 -0.05752039]\n",
      " ...\n",
      " [-0.41310154  0.07558908]\n",
      " [-0.40681537 -0.00923127]\n",
      " [ 0.59307753  0.0165655 ]] \n",
      "y_train:\n",
      " [1. 0. 0. ... 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into 80% training and 20% testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_pca, data[:,57], test_size=0.2, random_state=42)\n",
    "\n",
    "print('X_train:\\n', X_train, '\\ny_train:\\n', y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6bf66",
   "metadata": {},
   "source": [
    "You need to perform a K fold validation on this and report the average training error over all the k validations. \n",
    "- For this , you need to split the training data into k splits.\n",
    "- For each split, train a decision tree model and report the training , validation and test scores.\n",
    "- Report the scores in a tabular form for each validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ed15194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0308138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into k splits \n",
    "def k_fold_split(data, k):\n",
    "    # splitting the dataset into k folds\n",
    "    # returning a list of k folds\n",
    "    \n",
    "    data_split = []\n",
    "    data_copy = list(data)\n",
    "    fold_size = int(len(data) / k)\n",
    "    for i in range(k):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = np.random.randint(len(data_copy))\n",
    "            fold.append(data_copy.pop(index))\n",
    "        data_split.append(fold)\n",
    "    return data_split\n",
    "\n",
    "# find data-split\n",
    "\n",
    "data_split = k_fold_split(data_pca, 7)\n",
    "# print('Data Split:\\n', data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e981388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f81761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each split, train a decision tree model using DecisionTreeClassifier\n",
    "# use the above k_fold_split function to split the dataset into k splits\n",
    "# use the first k-1 splits to train the model and the last split to test the model\n",
    "\n",
    "def decision_tree_model(data, k):\n",
    "    # train a decision tree model using DecisionTreeClassifier\n",
    "    # return the accuracy of the model\n",
    "    data_split = k_fold_split(data, k)\n",
    "    scores = []\n",
    "    for i in range(len(data_split)):\n",
    "        train_set = list(data_split)\n",
    "        train_set.pop(i)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = []\n",
    "        for row in data_split[i]:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        y_train = [row[-1] for row in train_set]\n",
    "        X_train = [row[:-1] for row in train_set]\n",
    "        y_test = [row[-1] for row in test_set]\n",
    "        X_test = [row[:-1] for row in test_set]\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee802d60",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anjali\\OneDrive - International Institute of Information Technology\\Documents\\College Stuffs\\Third Year\\SMAI\\statistical-methods-in-AI-Monsoon-2022\\Assignments\\Assignment 3\\Assignment 3.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# finding training, validation and testing scores\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m scores \u001b[39m=\u001b[39m decision_tree_model(data_pca, \u001b[39m5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mScores:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, scores)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMean Accuracy: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39msum\u001b[39m(scores)\u001b[39m/\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39mlen\u001b[39m(scores))))\n",
      "\u001b[1;32mc:\\Users\\Anjali\\OneDrive - International Institute of Information Technology\\Documents\\College Stuffs\\Third Year\\SMAI\\statistical-methods-in-AI-Monsoon-2022\\Assignments\\Assignment 3\\Assignment 3.ipynb Cell 31\u001b[0m in \u001b[0;36mdecision_tree_model\u001b[1;34m(data, k)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X42sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m X_test \u001b[39m=\u001b[39m [row[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m test_set]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X42sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model \u001b[39m=\u001b[39m DecisionTreeClassifier()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X42sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X42sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X42sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, predictions)\n",
      "File \u001b[1;32mc:\\Users\\Anjali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    970\u001b[0m         X,\n\u001b[0;32m    971\u001b[0m         y,\n\u001b[0;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Anjali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:210\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m is_classification:\n\u001b[1;32m--> 210\u001b[0m     check_classification_targets(y)\n\u001b[0;32m    211\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(y)\n\u001b[0;32m    213\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Anjali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\multiclass.py:200\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    192\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\n\u001b[0;32m    194\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    195\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmultilabel-sequences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    199\u001b[0m ]:\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown label type: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "# finding training, validation and testing scores\n",
    "scores = decision_tree_model(data_pca, 5)\n",
    "print('Scores:\\n', scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63f99299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_split function row wise for a 2D matrix \n",
    "def test_split(index, value, data):\n",
    "    # split the dataset into two based on the index and value\n",
    "    # return two datasets\n",
    "    left, right = [], []\n",
    "    for row in data:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c167b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gini index function\n",
    "def gini_index(groups, classes):\n",
    "    # calculate the gini index for a split dataset\n",
    "    # return the gini index\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3e977ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each split, train a decision tree model using DescisionTreeClassifier from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7542a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, y_train, X_test, y_test):\n",
    "    # train a decision tree model using DescisionTreeClassifier from sklearn\n",
    "    # return the accuracy of the model\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "\n",
    "    # print('Predicted Values:\\n', y_pred)\n",
    "\n",
    "    tree.plot_tree(dt)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "604495ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Validation Accuracy:  1.0\n",
      "Training Accuracy:  1.0\n",
      "Validation Accuracy:  1.0\n",
      "Training Accuracy:  1.0\n",
      "Validation Accuracy:  1.0\n",
      "Test Accuracy:  1.0\n",
      "KNN Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize K and split the data\n",
    "k = 3\n",
    "kf = KFold(n_splits = k, shuffle = True, random_state = 42)\n",
    "n_neighbors = 7\n",
    "\n",
    "#Run the K fold Validation and report the scores\n",
    "# For each split, train a decision tree model and report the training, validation and test scores\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_train, X_test_val = X_train[train_index], X_train[test_index]\n",
    "    y_train_train, y_test_val = y_train[train_index], y_train[test_index]\n",
    "    knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "    knn.fit(X_train_train, y_train_train)\n",
    "    y_pred = knn.predict(X_test_val)\n",
    "    y_train_pred = knn.predict(X_train_train)\n",
    "\n",
    "    scores.append(accuracy_score(y_test_val, y_pred))\n",
    "\n",
    "    print('Training Accuracy: ', accuracy_score(y_train_train, y_train_pred))\n",
    "    print('Validation Accuracy: ', accuracy_score(y_test_val, y_pred))\n",
    "\n",
    "# Train the model on the training set and report the test score\n",
    "# YOUR CODE HERE\n",
    "knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "knn.fit(X_train, y_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "print('KNN Accuracy: ', np.mean(scores))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcdf68",
   "metadata": {},
   "source": [
    "### Question 4 Random Forest Algorithm\n",
    "**[30 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61115eaf",
   "metadata": {},
   "source": [
    "1. What is boosting, bagging and  stacking?\n",
    "Which class does random forests belong to and why? **[5 points]**\n",
    "\n",
    "<br><br>\n",
    "<b>Boosting: </b> It is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. <br>\n",
    "A random sample of data is selected, fitted with a model and then trained sequentially. \n",
    "<br><br>\n",
    "<b>Bagging: </b> It is the ensemble learning method that is commonly used to reduce variance within a noisy dataset. <br>\n",
    "A random sample of data in a training set is selected with replacement, i.e., the individual data points can be chosen more than once. These weak models are trained independently, and depending on it - regression or classification can be performed. <br>\n",
    "It helps in improving the performance and accuracy of algorithms. It is also known as Bootstrap Aggregation. \n",
    "<br><br>\n",
    "<b>Stacking: </b> It is the ensemble learning method which is used to predict multiple nodes to build a new model and improve model performance. <br>\n",
    "It basically helps us to train multiple models to solve similar problems, and based on their combined output, it builds a new model with improved performance. <br>\n",
    "It uses different ML models one after the other, wherein we add the predictions from each model to make a new feature. \n",
    "<br><br>\n",
    "Random forest algorithm is an extension of bagging method, and therefore, it is an ensemble learning method for classification, regression and other tasks. This comes under the class of bagging as it utilizes both bagging and feature randomness to create an uncorrelated forest of decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c366d",
   "metadata": {},
   "source": [
    "2. Implement random forest algorithm using different decision trees. **[25 points]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412cfb26",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anjali\\OneDrive - International Institute of Information Technology\\Documents\\College Stuffs\\Third Year\\SMAI\\statistical-methods-in-AI-Monsoon-2022\\Assignments\\Assignment 3\\Assignment 3.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X54sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m train_scores, val_scores, test_scores\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X54sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# implement random forest algorithm on the given dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X54sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# return the accuracy of the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X54sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m accuracy \u001b[39m=\u001b[39m random_forest_algorithm(data_norm, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1.0\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-Monsoon-2022/Assignments/Assignment%203/Assignment%203.ipynb#X54sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m'\u001b[39m, accuracy)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# Pass necessary params as per requirements\n",
    "    #######################\n",
    "    # Your code goes here #\n",
    "# implement random forest algorithm using different decision trees from scratch\n",
    "# implement the function using the above decision tree function\n",
    "# return the accuracy of the model\n",
    "def random_forest_algorithm(data, max_depth, min_size, sample_size, n_trees):\n",
    "    # find the training, validation and test scores\n",
    "    data_split = k_fold_split(data, 5)\n",
    "    train_scores, val_scores, test_scores = list(), list(), list()\n",
    "    for i in range(len(data_split)):\n",
    "        train_set, test_set = test_split(i, data_split)\n",
    "        train_set, val_set = test_split(i, train_set)\n",
    "        tree = decision_tree(X_train, y_train, X_test, y_test)\n",
    "        train_score = accuracy_score(train_set, tree)\n",
    "        val_score = accuracy_score(val_set, tree)\n",
    "        test_score = accuracy_score(test_set, tree)\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "        test_scores.append(test_score)\n",
    "    return train_scores, val_scores, test_scores\n",
    "\n",
    "# implement random forest algorithm on the given dataset\n",
    "# return the accuracy of the model\n",
    "accuracy = random_forest_algorithm(data_norm, 2, 1, 1.0, 5)\n",
    "print('Accuracy: ', accuracy)\n",
    "    #######################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4a32208fe57bd9bd2e8cf471b55207ccd1db41e4a4526642b234c85a6d3d80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
