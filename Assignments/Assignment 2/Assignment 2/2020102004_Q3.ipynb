{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389ddf1d",
   "metadata": {},
   "source": [
    "# Assignment 2 - Question 3\n",
    "The objective of this assignment is to get you familiarize with  the  problem  of  `Logistic Regression`.\n",
    "\n",
    "\n",
    "## Instructions\n",
    "- Do not Use Direct Inbuilt functions for the Task.\n",
    "- Numpy or other math libraries are allowed.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not change the contents of the Given cells. Use new cells to Write your code.\n",
    "\n",
    "\n",
    "## Submission\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Rename the notebook to `<roll_number>_Q3.ipynb`\n",
    "- Fill the Name and Roll number in the below markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6269f1",
   "metadata": {},
   "source": [
    "Name: Anjali Singh <br>\n",
    "Roll Number: 2020102004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850dba90",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Use the code below to load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225cdbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ed0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y =  make_blobs(n_samples=100, centers=[[2,4],[4,2]], random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2ec2927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14aee1abe20>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize dataset \n",
    "plt.plot(X[:,0][y==0],X[:,1][y==0],'o',color='red')\n",
    "plt.plot(X[:,0][y==1],X[:,1][y==1],'^',color='blue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43326e6b",
   "metadata": {},
   "source": [
    "Write your code below for logistic regression with Gradient Descent and plot the decision boundary.<br>\n",
    "Sample output is given in the file `LogisticRegression_sample_result.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a193966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalize feature matrix, X\n",
    "\n",
    "# logistic function: sigmoid function\n",
    "# f(x) = 1/(1+e^(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    f = 1.0/(1+np.exp(-x))\n",
    "\n",
    "    return f\n",
    "\n",
    "def log_gradient(X, y, beta):\n",
    "    m = len(y)\n",
    "    z = np.dot(X, beta.T)\n",
    "    A = sigmoid(z)\n",
    "    final_calc = (1/m)*np.dot(X, A-y)\n",
    "\n",
    "    return final_calc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad1f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# J for logistic regression is inversely proportional to the likelihood of parameters\n",
    "\n",
    "def costFunc(beta, X, y):\n",
    "    # J = cost function or loss function \n",
    "    z = np.dot(X, beta.T)\n",
    "    logisticFunc_v = sigmoid(z)\n",
    "    y = np.squeeze(y)\n",
    "\n",
    "    step1 = y*(np.log(logisticFunc_v))\n",
    "    step2 = (1-y)*(np.log(1 - logisticFunc_v))\n",
    "\n",
    "    final = - step1 - step2\n",
    "    final_mean = np.mean(final)\n",
    "\n",
    "    return final_mean\n",
    "\n",
    "def gradientFunc(X, y, beta, lr = 0.01, convergeChange = 0.001):\n",
    "    cost = costFunc(beta, X, y)\n",
    "    change_cost = 1\n",
    "    num_iterations = 1\n",
    "\n",
    "    while(change_cost > convergeChange):\n",
    "        old_cost = cost \n",
    "        beta = beta - (lr*log_gradient(X, y, beta).T)\n",
    "\n",
    "        cost = costFunc(beta, X, y)\n",
    "\n",
    "        change_cost = old_cost - cost\n",
    "        num_iterations += 1\n",
    "    \n",
    "    return beta, num_iterations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b5bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict values\n",
    "def predictValues(beta, X):\n",
    "    z = np.dot(X.any(), beta)\n",
    "    pred_prob = sigmoid(z)\n",
    "    # pred_prob = logisticFunc(beta, X.any())\n",
    "\n",
    "    if (pred_prob.all() >= 0.5):\n",
    "        pred_value = 1\n",
    "    else:\n",
    "        pred_value = 0\n",
    "    \n",
    "    # np.squeeze is used where we want the 2D array from a 3D array\n",
    "    return np.squeeze(pred_value)\n",
    "\n",
    "# function to plot the decision boundary\n",
    "\n",
    "def plot_reg(X, y, beta):\n",
    "    x_0 = X[np.where(y == 0.0)]\n",
    "    x_1 = X[np.where(y == 1.0)]\n",
    "\n",
    "    x1 = np.arange(0, 1, 0.1)\n",
    "    x2 = -(beta[0, 0] + beta[0, 1]*x1)/beta[0, 2]\n",
    "\n",
    "    plt.plot(X[:,0][y==0], X[:,1][y==0], 'o', color = 'red')\n",
    "    plt.plot(X[:,0][y==1], X[:,1][y==1], '^', color = 'blue')\n",
    "\n",
    "    plt.plot(x1, x2, c = 'k', label = 'Regression Line')\n",
    "\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ad2942",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (100,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anjali\\OneDrive - International Institute of Information Technology\\Documents\\College Stuffs\\Third Year\\SMAI\\statistical-methods-in-AI-monsoon-2022\\Assignments\\Assignment 2\\Assignment 2\\2020102004_Q3.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m beta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# beta = np.matrix(np.zeros(X.shape[1]))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# beta values after running gradient descent\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m beta, num_iter \u001b[39m=\u001b[39m gradientFunc(X, y, beta)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# estimated beta values and number of iterations\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEstimated regression coefficients:\u001b[39m\u001b[39m\"\u001b[39m, beta)\n",
      "\u001b[1;32mc:\\Users\\Anjali\\OneDrive - International Institute of Information Technology\\Documents\\College Stuffs\\Third Year\\SMAI\\statistical-methods-in-AI-monsoon-2022\\Assignments\\Assignment 2\\Assignment 2\\2020102004_Q3.ipynb Cell 11\u001b[0m in \u001b[0;36mgradientFunc\u001b[1;34m(X, y, beta, lr, convergeChange)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradientFunc\u001b[39m(X, y, beta, lr \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m, convergeChange \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     cost \u001b[39m=\u001b[39m costFunc(beta, X, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     change_cost \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     num_iterations \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\Anjali\\OneDrive - International Institute of Information Technology\\Documents\\College Stuffs\\Third Year\\SMAI\\statistical-methods-in-AI-monsoon-2022\\Assignments\\Assignment 2\\Assignment 2\\2020102004_Q3.ipynb Cell 11\u001b[0m in \u001b[0;36mcostFunc\u001b[1;34m(beta, X, y)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcostFunc\u001b[39m(beta, X, y):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# J = cost function or loss function \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X, beta\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     logisticFunc_v \u001b[39m=\u001b[39m sigmoid(z)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anjali/OneDrive%20-%20International%20Institute%20of%20Information%20Technology/Documents/College%20Stuffs/Third%20Year/SMAI/statistical-methods-in-AI-monsoon-2022/Assignments/Assignment%202/Assignment%202/2020102004_Q3.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(y)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (100,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # .T: takes the transpose \n",
    "    # X = normalize(X)\n",
    "    X = np.hstack((np.matrix(np.ones(X.shape[0])).T, X))\n",
    "\n",
    "    y = X[:, -1]\n",
    " \n",
    "    # initial beta values\n",
    "    beta = np.zeros((X.shape[1], 1))\n",
    "    # beta = np.matrix(np.zeros(X.shape[1]))\n",
    " \n",
    "    # beta values after running gradient descent\n",
    "    beta, num_iter = gradientFunc(X, y, beta)\n",
    " \n",
    "    # estimated beta values and number of iterations\n",
    "    print(\"Estimated regression coefficients:\", beta)\n",
    "    print(\"No. of iterations:\", num_iter)\n",
    " \n",
    "    # predicted labels\n",
    "    y_pred = predictValues(beta, X)\n",
    "     \n",
    "    # number of correctly predicted labels\n",
    "    print(\"Correctly predicted labels:\", np.sum(y == y_pred))\n",
    "     \n",
    "    # plotting regression line\n",
    "    plot_reg(X, y, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f2b4c5",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1) Explain the approach of Logistic Regression.<br>\n",
    "2) What is the loss function used?<br>\n",
    "3) Explain if we can use Mean Square Error(MSE) as Loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f2cef",
   "metadata": {},
   "source": [
    "<b> Q1. Explain the approach of Logistic Regression. </b> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679a4a4",
   "metadata": {},
   "source": [
    "<b> Q2. What is the loss function used? </b> <br>\n",
    "\n",
    "The loss function used for logistic regression is as follows: - <br>\n",
    "\n",
    "$$ \n",
    "cost(h_{\\theta}(x), y) = {\\begin{cases} -log(h_{\\theta}(x)) \\space\\space\\space\\space if \\space\\space y = 1 \\\\ -log(1-h_{\\theta}(x)) \\space\\space\\space\\space if \\space\\space y = 0 \\end{cases}}  \n",
    "$$\n",
    "\n",
    "This gives us a non-convex loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa41a7",
   "metadata": {},
   "source": [
    "<b> Q3. Explain if we can use Mean Square Error (MSE) as loss function. </b> <br>\n",
    "\n",
    "No, we cannot use the MSE as loss function. This is because, we use the sigmoid function in logistic regression and we do non-linear transformations to obtain the probabiities. Using MSE, we'll need to square the non-linear transformations which leads to non-convexity with local minimums. And thereby, it becomes almost next to impossible to find the global minimum using gradient descent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4a32208fe57bd9bd2e8cf471b55207ccd1db41e4a4526642b234c85a6d3d80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
